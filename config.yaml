# Transformer Training Configuration

# Model Architecture
model:
  vocab_size: 5001  # Will be set dynamically from tokenizer
  d_model: 64
  n_heads: 4
  num_encoder_layers: 2
  num_decoder_layers: 2
  max_len: 64
  dim_feedforward: 256
  dropout: 0.1

# Training
training:
  num_epochs: 5
  batch_size: 8
  learning_rate: 0.0001
  weight_decay: 0.01
  warmup_steps: 100
  max_grad_norm: 1.0

# Data
data:
  train_path: "data/cleaned_wmt17_de_en_split_train.json"
  val_path: "data/cleaned_wmt17_de_en_split_validation.json"
  test_path: "data/cleaned_wmt17_de_en_split_test.json"
  tokenizer_path: "data/small_bpe_tokenizer"
  num_train_samples: null  # null = use full dataset
  num_val_samples: null
  num_test_samples: null
  max_src_len: 32
  max_tgt_len: 32

# Generation
generation:
  max_length: 50
  method: "greedy"  # greedy, beam, sampling

# Paths
paths:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  output_dir: "outputs"

# Weights & Biases
wandb:
  enabled: true
  project: "transformer-translation"

# Reproducibility
seed: 42
