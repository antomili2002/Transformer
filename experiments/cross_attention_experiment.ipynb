{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Cross-Attention Pattern Analysis\n",
    "\n",
    "## Theory\n",
    "\n",
    "In sequence-to-sequence transformers, **cross-attention** in the decoder allows each target token to attend to all source tokens. This mechanism learns implicit **word alignment** between source and target languages.\n",
    "\n",
    "**Multi-head attention** with $h$ heads:\n",
    "- Attention weights: $\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "- Output: weighted sum of values based on query-key similarity\n",
    "\n",
    "**What we'll visualize:**\n",
    "1. Single-head attention showing word-to-word alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from modelling.transformer_preln import PreLNTransformer\n",
    "from modelling.dataloader import MyBPETokenizer\n",
    "\n",
    "plt.rcParams['font.size'] = 9\n",
    "plt.rcParams['axes.labelsize'] = 9\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 7\n",
    "plt.rcParams['ytick.labelsize'] = 7\n",
    "plt.rcParams['legend.fontsize'] = 8\n",
    "\n",
    "SINGLE_COL_WIDTH = 3.5\n",
    "DOUBLE_COL_WIDTH = 7.0\n",
    "FIG_HEIGHT = 2.5\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "save_dir = '../figures/experiments/cross_attention/'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer from ../data/bpe_tokenizer\n",
      "Vocabulary size: 37000\n",
      "BOS token: [BOS] (ID: 1)\n",
      "EOS token: [EOS] (ID: 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MyBPETokenizer(\n",
    "    texts=[\"dummy\"],\n",
    "    vocab_size=37000,\n",
    "    save_dir='../data/bpe_tokenizer'\n",
    ")\n",
    "\n",
    "vocab_size = tokenizer.tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"BOS token: {tokenizer.bos} (ID: {tokenizer.bos_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos} (ID: {tokenizer.eos_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with 82,028,680 parameters\n"
     ]
    }
   ],
   "source": [
    "model = PreLNTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    max_len=128,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load('../checkpoints_preln/best_model.pt', map_location=device, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_attention_weights(model, src_text, max_len=50):\n",
    "    \"\"\"\n",
    "    Generate translation and extract cross-attention weights by manually computing them.\n",
    "    No hooks or model modification needed - we compute attention directly.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    src_ids = tokenizer.encode_src(src_text, max_length=128).unsqueeze(0).to(device)\n",
    "    src_mask = (src_ids != tokenizer.pad_id).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_ids, src_mask)\n",
    "    \n",
    "    # Start with BOS token\n",
    "    tgt = torch.tensor([[tokenizer.bos_id]], dtype=torch.long, device=device)\n",
    "    all_attentions = []\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for step in range(max_len - 1):\n",
    "        with torch.no_grad():\n",
    "            # Embed target\n",
    "            tgt_embedded = model.embedding(tgt) * (model.d_model ** 0.5)\n",
    "            tgt_embedded = model.positional_encoding(tgt_embedded)\n",
    "            \n",
    "            x = tgt_embedded\n",
    "            \n",
    "            # Go through decoder layers manually\n",
    "            layer_attentions = []\n",
    "            for layer in model.decoder_layers:\n",
    "                # Self-attention\n",
    "                x_norm = layer.layer_norm_1(x)\n",
    "                self_attn_out = layer.self_attention(x_norm, x_norm, x_norm, attention_mask=None, future_mask=True)\n",
    "                x = x + self_attn_out\n",
    "                \n",
    "                # Cross-attention - MANUALLY COMPUTE ATTENTION WEIGHTS\n",
    "                x_norm = layer.layer_norm_2(x)\n",
    "                \n",
    "                # Extract Q, K, V\n",
    "                q = layer.encoder_attention.query_transform(x_norm)\n",
    "                k = layer.encoder_attention.key_transform(memory)\n",
    "                v = layer.encoder_attention.value_transform(memory)\n",
    "                \n",
    "                B, Tq, _ = q.shape\n",
    "                _, Tk, _ = k.shape\n",
    "                \n",
    "                # Reshape to multi-head format\n",
    "                num_heads = layer.encoder_attention.num_head\n",
    "                head_dim = layer.encoder_attention.head_dim\n",
    "                \n",
    "                q = q.view(B, Tq, num_heads, head_dim).permute(0, 2, 1, 3)  # [B, h, Tq, d_k]\n",
    "                k = k.view(B, Tk, num_heads, head_dim).permute(0, 2, 1, 3)  # [B, h, Tk, d_k]\n",
    "                v = v.view(B, Tk, num_heads, head_dim).permute(0, 2, 1, 3)  # [B, h, Tk, d_k]\n",
    "                \n",
    "                # Compute attention scores\n",
    "                scores = (q @ k.transpose(-2, -1)) / np.sqrt(head_dim)\n",
    "                \n",
    "                # Apply source mask\n",
    "                if src_mask is not None:\n",
    "                    key_pad = (src_mask.squeeze(1) == 0).unsqueeze(1).unsqueeze(1)  # [B, 1, 1, Tk]\n",
    "                    scores = scores.masked_fill(key_pad, float(\"-inf\"))\n",
    "                \n",
    "                # Attention weights\n",
    "                attn_weights = torch.softmax(scores, dim=-1)  # [B, h, Tq, Tk]\n",
    "                \n",
    "                # Store attention for last query position only\n",
    "                layer_attentions.append(attn_weights[:, :, -1:, :].cpu())  # [B, h, 1, Tk]\n",
    "                \n",
    "                # Continue with actual cross-attention output\n",
    "                cross_attn_out = layer.encoder_attention(x_norm, memory, memory, \n",
    "                                                         attention_mask=src_mask, future_mask=False)\n",
    "                x = x + cross_attn_out\n",
    "                \n",
    "                # FFN\n",
    "                x_norm = layer.layer_norm_3(x)\n",
    "                ffn_out = layer.feature_transformation(x_norm)\n",
    "                x = x + ffn_out\n",
    "            \n",
    "            # Final norm and projection\n",
    "            x = model.decoder_final_norm(x)\n",
    "            logits = model.output_projection(x)\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            \n",
    "            # Stack layer attentions: [num_layers, B, h, 1, Tk]\n",
    "            all_attentions.append(torch.stack(layer_attentions, dim=0))\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_id:\n",
    "                break\n",
    "    \n",
    "    # Process attentions: concatenate across target positions\n",
    "    # all_attentions is a list of [num_layers, B, h, 1, Tk] tensors\n",
    "    attentions = torch.cat(all_attentions, dim=3)  # [num_layers, B, h, total_tgt_len, Tk]\n",
    "    attentions = attentions.squeeze(1)  # Remove batch dim: [num_layers, h, total_tgt_len, Tk]\n",
    "    \n",
    "    # Decode tokens\n",
    "    src_tokens = tokenizer.decode(src_ids[0].cpu().tolist())\n",
    "    tgt_tokens = tokenizer.decode(tgt[0].cpu().tolist())\n",
    "    \n",
    "    return tgt_tokens, src_tokens, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Translation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected for visualization\n",
      "Source (DE): Er war ein freundlicher Mensch mit einem großen Herzen.\n",
      "Reference (EN): He was a kind spirit with a big heart.\n",
      "Prediction: he was a friendly man with a great heart .\n"
     ]
    }
   ],
   "source": [
    "with open('../data/cleaned_wmt17_de_en_split_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "#for example_idx, example in enumerate(test_data[:200]):\n",
    "#    src_text = example['src']\n",
    "#    tgt_reference = example['tgt']\n",
    "#    \n",
    "#    tgt_tokens, src_tokens, attentions = get_cross_attention_weights(model, src_text)\n",
    "#    \n",
    "#    print(f\"Example {example_idx}\")\n",
    "#    print(f\"Source (DE): {src_text}\")\n",
    "#    print(f\"Reference (EN): {tgt_reference}\")\n",
    "#    print(f\"Prediction: {tgt_tokens}\")\n",
    "\n",
    "# best one for visualization\n",
    "example_idx = 12 \n",
    "example = test_data[example_idx]\n",
    "src_text = example['src']\n",
    "tgt_reference = example['tgt']\n",
    "\n",
    "tgt_tokens, src_tokens, attentions = get_cross_attention_weights(model, src_text)\n",
    "\n",
    "print(f\"Selected for visualization\")\n",
    "print(f\"Source (DE): {src_text}\")\n",
    "print(f\"Reference (EN): {tgt_reference}\")\n",
    "print(f\"Prediction: {tgt_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered attention shape: (10, 10)\n",
      "Source tokens: 10, Target tokens: 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_idx = 4\n",
    "head_idx = 5\n",
    "\n",
    "attn_matrix = attentions[layer_idx, head_idx].squeeze().numpy()\n",
    "\n",
    "src_ids = tokenizer.encode_src(src_text, max_length=128)\n",
    "actual_src_len = (src_ids != tokenizer.pad_id).sum().item()\n",
    "attn_matrix = attn_matrix[:, :actual_src_len]\n",
    "\n",
    "# Get token lists and filter special tokens\n",
    "src_tokens_list = [tokenizer.tokenizer.id_to_token(id.item()) for id in src_ids[:actual_src_len]]\n",
    "tgt_ids = tokenizer.encode_tgt(tgt_tokens, max_length=128)\n",
    "tgt_tokens_list = [tokenizer.tokenizer.id_to_token(id.item()) for id in tgt_ids \n",
    "                   if id.item() not in [tokenizer.pad_id, tokenizer.bos_id, tokenizer.eos_id]]\n",
    "\n",
    "# Remove special tokens from display\n",
    "src_display = []\n",
    "src_indices = []\n",
    "for i, t in enumerate(src_tokens_list):\n",
    "    if t not in ['[BOS]', '[EOS]', '[PAD]']:\n",
    "        src_display.append(t.replace('</w>', '·'))\n",
    "        src_indices.append(i)\n",
    "\n",
    "tgt_display = []\n",
    "tgt_indices = []\n",
    "for i, t in enumerate(tgt_tokens_list):\n",
    "    if t not in ['[BOS]', '[EOS]', '[PAD]']:\n",
    "        tgt_display.append(t.replace('</w>', '·'))\n",
    "        tgt_indices.append(i)\n",
    "\n",
    "# Filter attention matrix to match\n",
    "attn_filtered = attn_matrix[np.ix_(tgt_indices, src_indices)]\n",
    "\n",
    "print(f\"Filtered attention shape: {attn_filtered.shape}\")\n",
    "print(f\"Source tokens: {len(src_display)}, Target tokens: {len(tgt_display)}\")\n",
    "\n",
    "# Single column figure\n",
    "fig, ax = plt.subplots(figsize=(3.5, 3.0))\n",
    "im = ax.imshow(attn_filtered, cmap='Blues', aspect='auto')\n",
    "\n",
    "# Smaller fonts, no axis labels\n",
    "ax.set_xticks(range(len(src_display)))\n",
    "ax.set_xticklabels(src_display, rotation=90, ha='center', fontsize=6)\n",
    "ax.set_yticks(range(len(tgt_display)))\n",
    "ax.set_yticklabels(tgt_display, fontsize=6)\n",
    "\n",
    "# Title only (no axis labels)\n",
    "ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}', fontsize=8, pad=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir + 'cross_attention_single_head.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
