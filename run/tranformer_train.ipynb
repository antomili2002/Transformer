{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d328455e",
   "metadata": {},
   "source": [
    "## Practical 9\n",
    "### Exercises\n",
    "\n",
    "1. Initialise a small version of your transformer model (do not use more than 4 layers and 64 hidden units unless you have access to sufficient compute).\n",
    "2. Initialise the dataloader using the dataset class from practical 5.\n",
    "3. Initialise the loss function (cross-entropy loss) optimiser and learning rate scheduler.\n",
    "4. Implement the training loop.\n",
    "5. Train the model for 5 epochs and ensure that loss decreases for both the training and validation sets of the dataset. You can use a small randomly selected subset of the training data to speed up training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217bce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "# use realtive import\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from modelling.model import Transformer\n",
    "from modelling.dataloader import TranslationDataset, MyBPETokenizer\n",
    "from modelling.optimizer import create_adamw_optimizer\n",
    "from modelling.scheduler import TransformerScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3df39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train samples: 4801360\n",
      "Val samples: 2782\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "print(\"Loading data...\")\n",
    "with open(\"../data/cleaned_wmt17_de_en_split_train.json\", encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(\"../data/cleaned_wmt17_de_en_split_validation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(\"../data/cleaned_wmt17_de_en_texts_for_tokenizer.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_texts = json.load(f)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "print(f\"Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a862587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10000 training samples and 2782 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# use small subset for training on CPU for 5 epochs\n",
    "# increased the number of samples because of overfitting with too small data\n",
    "num_train_samples = 10000\n",
    "num_val_samples = 3000\n",
    "\n",
    "train_data = train_data[:num_train_samples]\n",
    "val_data = val_data[:num_val_samples]\n",
    "\n",
    "tokenizer_texts = tokenizer_texts[:20000]\n",
    "\n",
    "print(f\"Using {len(train_data)} training samples and {len(val_data)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9208d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer from ../data/small_bpe_tokenizer\n"
     ]
    }
   ],
   "source": [
    "# train Tokenizer\n",
    "tokenizer = MyBPETokenizer(texts=tokenizer_texts, vocab_size=5000, save_dir=\"../data/small_bpe_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f766cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and Dataloaders initialized.\n",
      "Train dataset size: 10000\n",
      "Validation dataset size: 2782\n"
     ]
    }
   ],
   "source": [
    "# instanciate dataset and dataloader\n",
    "train_dataset = TranslationDataset(train_data, tokenizer, max_src_len=32, max_tgt_len=32)\n",
    "val_dataset = TranslationDataset(val_data, tokenizer, max_src_len=32, max_tgt_len=32)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Datasets and Dataloaders initialized.\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7346f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=5000,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    max_len=64,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d867857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 876936\n"
     ]
    }
   ],
   "source": [
    "# count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cccda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id)\n",
    "\n",
    "# added weight decay for better generalization\n",
    "optimizer = create_adamw_optimizer(model, learning_rate=1e-4, weight_decay=1e-4)\n",
    "scheduler = TransformerScheduler(optimizer, d_model=64, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, epoch_num):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Training Epoch {epoch_num}\")):\n",
    "        src_ids = batch['src_ids'].to(device)\n",
    "        tgt_ids = batch['tgt_ids'].to(device)\n",
    "\n",
    "        tgt_input = tgt_ids[:, :-1] # remove last token for decoder input\n",
    "        tgt_output = tgt_ids[:, 1:] # remove first token for target output\n",
    "        \n",
    "        #if batch_idx == 0 and epoch_num % 1 == 0:\n",
    "        #    print(f\"Batch {batch_idx} shapes\")\n",
    "        #    print(f\"src_ids: {src_ids.shape}\")\n",
    "        #    print(f\"tgt_ids: {tgt_ids.shape}\")\n",
    "        \n",
    "        #if batch_idx == 0 and epoch_num % 1 == 0:\n",
    "        #    print(f\"tgt_input: {tgt_input.shape}\")\n",
    "        #    print(f\"tgt_output: {tgt_output.shape}\")\n",
    "        \n",
    "        # create padding masks\n",
    "        src_mask = (src_ids != tokenizer.pad_id).to(device)  # [batch_size, src_len]\n",
    "        tgt_mask = (tgt_input != tokenizer.pad_id).to(device)  # [batch_size, tgt_len]\n",
    "        memory_mask = src_mask  # [batch_size, src_len]\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_ids, tgt_input,\n",
    "                       src_mask=src_mask,\n",
    "                       tgt_mask=tgt_mask,\n",
    "                       memory_mask=memory_mask)\n",
    "        \n",
    "        #if batch_idx == 0 and epoch_num % 1 == 0:\n",
    "        #    print(f\"Model output shape: {output.shape}\")\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        # CrossEntropyLoss expects: input=[N, C], target=[N]\n",
    "        # where N = batch_size * seq_len, C = vocab_size\n",
    "        batch_size, seq_len, vocab_size = output.shape\n",
    "        output_reshaped = output.reshape(-1, vocab_size)      # [batch_size * seq_len, vocab_size]\n",
    "        tgt_output_reshaped = tgt_output.reshape(-1)          # [batch_size * seq_len]\n",
    "        \n",
    "        #if batch_idx == 0:\n",
    "        #    print(f\"\\nAfter reshaping for loss:\")\n",
    "        #    print(f\"output_reshaped shape: {output_reshaped.shape}\")           # [batch*seq, vocab]\n",
    "        #    print(f\"tgt_output_reshaped shape: {tgt_output_reshaped.shape}\")   # [batch*seq]\n",
    "        #    print(f\"vocab_size: {vocab_size}\")\n",
    "        #print(f\"Output reshaped shape: {output_reshaped.shape}, Target reshaped shape: {tgt_output_reshaped.shape}, Vocab size: {vocab_size}\")\n",
    "        loss = criterion(output_reshaped, tgt_output_reshaped)\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = total_loss / batch_count\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d6dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, epoch_num):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Validation Epoch {epoch_num}\")):\n",
    "            src_ids = batch[\"src_ids\"].to(device)\n",
    "            tgt_ids = batch[\"tgt_ids\"].to(device)\n",
    "            \n",
    "            # Same teacher forcing setup as training\n",
    "            tgt_input = tgt_ids[:, :-1]\n",
    "            tgt_output = tgt_ids[:, 1:]\n",
    "            \n",
    "            # create padding masks same as training\n",
    "            src_mask = (src_ids != tokenizer.pad_id).to(device)\n",
    "            tgt_mask = (tgt_input != tokenizer.pad_id).to(device)\n",
    "            memory_mask = src_mask\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src_ids, tgt_input, \n",
    "                           src_mask=src_mask, \n",
    "                           tgt_mask=tgt_mask, \n",
    "                           memory_mask=memory_mask)\n",
    "            \n",
    "            # Reshape for loss\n",
    "            vocab_size = output.shape[-1]\n",
    "            output_reshaped = output.reshape(-1, vocab_size)\n",
    "            tgt_output_reshaped = tgt_output.reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output_reshaped, tgt_output_reshaped)\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "    \n",
    "    avg_loss = total_loss / batch_count\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a692fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch+1)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, device, epoch+1)\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9775b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, num_epochs):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    \n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Loss', linewidth=2, markersize=8)\n",
    "    plt.plot(epochs, val_losses, 'r-s', label='Validation Loss', linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('Training and Validation Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(epochs)\n",
    "    \n",
    "    # Add value labels on points\n",
    "    for i, (tl, vl) in enumerate(zip(train_losses, val_losses)):\n",
    "        plt.text(i+1, tl, f'{tl:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        plt.text(i+1, vl, f'{vl:.3f}', ha='center', va='top', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Final Training Loss:   {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673ed9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e5d6cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 1250/1250 [04:16<00:00,  4.87it/s] \n",
      "Validation Epoch 1: 100%|██████████| 348/348 [00:00<00:00, 444.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "Train Loss: 3.2506\n",
      "Val Loss: 4.6668\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 1250/1250 [00:09<00:00, 132.77it/s]\n",
      "Validation Epoch 2: 100%|██████████| 348/348 [00:00<00:00, 446.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary:\n",
      "Train Loss: 3.2220\n",
      "Val Loss: 4.6532\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 1250/1250 [00:09<00:00, 131.54it/s]\n",
      "Validation Epoch 3: 100%|██████████| 348/348 [00:00<00:00, 446.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Summary:\n",
      "Train Loss: 3.2013\n",
      "Val Loss: 4.6510\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 1250/1250 [00:09<00:00, 135.13it/s]\n",
      "Validation Epoch 4: 100%|██████████| 348/348 [00:00<00:00, 443.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Summary:\n",
      "Train Loss: 3.1790\n",
      "Val Loss: 4.6607\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  69%|██████▊   | 858/1250 [00:06<00:02, 134.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# run the training\u001b[39;00m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m plot_losses(train_losses, val_losses, num_epochs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     15\u001b[39m val_loss = validate(model, val_loader, criterion, device, epoch+\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, scheduler, device, epoch_num)\u001b[39m\n\u001b[32m     54\u001b[39m loss = criterion(output_reshaped, tgt_output_reshaped)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# backward pass and optimization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m optimizer.step()\n\u001b[32m     59\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformer/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformer/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/transformer/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run the training\n",
    "num_epochs = 5\n",
    "train_losses, val_losses = train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs=num_epochs\n",
    ")\n",
    "plot_losses(train_losses, val_losses, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8cfd1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tokenizer vocab size: 5000\n"
     ]
    }
   ],
   "source": [
    "actual_vocab_size = len(tokenizer.tokenizer.get_vocab())\n",
    "print(f\"Actual tokenizer vocab size: {actual_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63412f87",
   "metadata": {},
   "source": [
    "### Practical 10\n",
    "1. Implement the autoregressive generation procedure described above using your transformer mode(Using greedy decoding, remember to add a maximum length to the generation procedure to prevent infinite generation.)\n",
    "2. Generate translations for the test set (or a subset of the test set) of WMT17 German-English.\n",
    "3. Evaluate the BLEU score of your model on the test set (or a subset of the test set) of WMT17 German-English.\n",
    "4. Evaluate some of the translations generated by your model. Do they make sense? What are some of the errors made by your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8d54c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modelling.generation import evaluate_bleu, generate_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e5d0f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/cleaned_wmt17_de_en_split_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f206f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples available: 2761\n",
      "Using 200 samples for evaluation\n"
     ]
    }
   ],
   "source": [
    "num_test_samples = 200\n",
    "test_data_samples = test_data[:num_test_samples]\n",
    "\n",
    "print(f\"Total test samples available: {len(test_data)}\")\n",
    "print(f\"Using {len(test_data_samples)} samples for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94693576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence: 28-jähriger Koch in San Francisco Mall tot aufgefunden\n",
      "Generated Translation: ('i wo ul d li ke to sa y t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d', [1, 37, 264, 822, 32, 146, 445, 717, 144, 53, 48, 221, 613, 4136, 4251, 47, 396, 822, 32, 146, 445, 717, 86, 48, 221, 613, 4136, 4251, 47, 396, 822, 32])\n"
     ]
    }
   ],
   "source": [
    "src_sentence = test_data_samples[0]['src']\n",
    "    \n",
    "generated_translation = generate_translation(\n",
    "    model,\n",
    "    src_sentence,\n",
    "    tokenizer,\n",
    "    max_len=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Source Sentence: {src_sentence}\")\n",
    "print(f\"Generated Translation: {generated_translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a9bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations for 200 samples...\n",
      "Progress: 10/200\n",
      "Progress: 20/200\n",
      "Progress: 30/200\n",
      "Progress: 40/200\n",
      "Progress: 50/200\n",
      "Progress: 60/200\n",
      "Progress: 70/200\n",
      "Progress: 80/200\n",
      "Progress: 90/200\n",
      "Progress: 100/200\n",
      "Progress: 110/200\n",
      "Progress: 120/200\n",
      "Progress: 130/200\n",
      "Progress: 140/200\n",
      "Progress: 150/200\n",
      "Progress: 160/200\n",
      "Progress: 170/200\n",
      "Progress: 180/200\n",
      "Progress: 190/200\n",
      "Progress: 200/200\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_bleu(\n",
    "    model,\n",
    "    test_data_samples,\n",
    "    tokenizer,\n",
    "    max_len=32,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f0a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score on Test Set: 0.00\n",
      "Example Translations:\n",
      "Source: 28-jähriger Koch in San Francisco Mall tot aufgefunden\n",
      "Target: 28-Year-Old Chef Found Dead at San Francisco Mall\n",
      "Predicted: i wo ul d li ke to sa y t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d\n",
      "Source: Ein 28-jähriger Koch, der vor kurzem nach San Francisco gezogen ist, wurde im Treppenhaus eines örtlichen Einkaufzentrums tot aufgefunden.\n",
      "Target: A 28-year-old chef who had recently moved to San Francisco was found dead in the stairwell of a local mall this week.\n",
      "Predicted: i wo ul d li ke to be t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d li\n",
      "Source: Der Bruder des Opfers sagte aus, dass er sich niemanden vorstellen kann, der ihm schaden wollen würde,  Endlich ging es bei ihm wieder bergauf. \n",
      "Target: But the victim s brother says he can t think of anyone who would want to hurt him, saying,  Things were finally going well for him. \n",
      "Predicted: i wo ul d li ke to sa y t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d\n",
      "Source: Der am Mittwoch morgen in der Westfield Mall gefundene Leichnam wurde als der 28 Jahre alte Frank Galicia aus San Francisco identifiziert, teilte die gerichtsmedizinische Abteilung in San Francisco mit.\n",
      "Target: The body found at the Westfield Mall Wednesday morning was identified as 28-year-old San Francisco resident Frank Galicia, the San Francisco Medical Examiner s Office said.\n",
      "Predicted: i wo ul d li ke to sa y t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d\n",
      "Source: Das San Francisco Police Department sagte, dass der Tod als Mord eingestuft wurde und die Ermittlungen am Laufen sind.\n",
      "Target: The San Francisco Police Department said the death was ruled a homicide and an investigation is ongoing.\n",
      "Predicted: i wo ul d li ke to sa y t hat the com mission s ho ul d li ke to be t hat the com mission s ho ul d\n"
     ]
    }
   ],
   "source": [
    "# print BLEU score and show 5 example translations\n",
    "print(f\"BLEU Score on Test Set: {result['bleu_score']:.2f}\")\n",
    "\n",
    "print(\"Example Translations:\")\n",
    "for i in range(5):\n",
    "    print(f\"Source: {result['source_texts'][i]}\")\n",
    "    print(f\"Target: {result['references'][i]}\")\n",
    "    print(f\"Predicted: {result['predictions'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f9781",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- The model is extremly small and trained for very few epochs, so the BLEU score is expected to be low.\n",
    "- The only produces [UNK] tokens as output, indicating that it has not learned to generate meaningful translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c493c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
