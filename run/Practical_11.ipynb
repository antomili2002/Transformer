{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e50296c",
   "metadata": {},
   "source": [
    "# Implementing Transformer Models\n",
    "## Practical XI\n",
    "Carel van Niekerk & Hsien-Chin Lin\n",
    "\n",
    "19-23.01.2026\n",
    "\n",
    "---\n",
    "\n",
    "In this practical we will implement GPU training and mixed precision training for a simple entailment model. We will use the [Huggingface Transformers](https://huggingface.co/transformers/) library to implement the model and [Datasets](https://huggingface.co/docs/datasets/) to load the data. We will use the [QNLI](https://huggingface.co/datasets/viewer/?dataset=glue&config=qnli) dataset from the [GLUE](https://huggingface.co/datasets/glue) benchmark. The QNLI dataset is a simple entailment task where the model must predict whether a sentence entails a question. For example, the sentence 'The dog is playing with a ball' entails the question 'Is the dog playing with a ball?'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b726022",
   "metadata": {},
   "source": [
    "#### 1. Getting Started\n",
    "\n",
    "We will use the Google Collab environment for this practical. To get started download this notebook and upload it in a Google Collab session. You will also need to change the runtime type to a 'Python 3: T4 GPU' session, this can be done using the 'change runtime type' option in the 'Runtime' dropdown menu. Once this is done you can run the following code to install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97403dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/amilitello/miniconda3/envs/transformer/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59129713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "from transformers import (RobertaForSequenceClassification, RobertaTokenizer,\n",
    "                          get_linear_schedule_with_warmup)\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import time\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92de8f6",
   "metadata": {},
   "source": [
    "#### 2. Dataset and Model\n",
    "\n",
    "We will use a simple Roberta for Sequence Classification model to learn the entailment task presented in the QNLI GLUE task. This is a simple binary classification task.\n",
    "\n",
    "##### 2.1 Initialising the model and tokenizer\n",
    "\n",
    "Here we initialise the Roberta model and its accompanying tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7583360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'roberta-base'\n",
    "MAX_INPUT_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "TRAINING_EPOCHS = 2\n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "MIXED_PRECISION_TRAINING = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22b2674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf094b4",
   "metadata": {},
   "source": [
    "##### 2.2. Loading and preparing the dataset\n",
    "\n",
    "We will load the dataset from the datasets library and tokenize all observations to prepare them for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d249c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_dataset = load_dataset('glue', 'qnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "011d265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert example to features.\n",
    "    \n",
    "    Args:\n",
    "        example (dict): Example from the QNLI dataset.\n",
    "    Returns:\n",
    "        features (dict): Features for the example.\n",
    "    \"\"\"\n",
    "    features = tokenizer(example['question'], example['sentence'], max_length=MAX_INPUT_LENGTH,\n",
    "                         padding='max_length', truncation='longest_first')\n",
    "\n",
    "    features['labels'] = example['label']\n",
    "\n",
    "    return features\n",
    "\n",
    "def collate(batch: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to collate the batch.\n",
    "    \n",
    "    Args:\n",
    "        batch (list): List of examples from the QNLI dataset.\n",
    "    Returns:\n",
    "        features (dict): Features for the batch.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'input_ids': torch.tensor([itm['input_ids'] for itm in batch]),\n",
    "        'attention_mask': torch.tensor([itm['attention_mask'] for itm in batch]),\n",
    "        'labels': torch.tensor([itm['labels'] for itm in batch]),\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aabaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the datasets\n",
    "train_dataset = qnli_dataset['train'].map(convert_example_to_features)\n",
    "validation_dataset = qnli_dataset['validation'].map(convert_example_to_features)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn = collate)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, collate_fn = collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4765",
   "metadata": {},
   "source": [
    "#### 3. Training the Model\n",
    "\n",
    "To train the model we initialise the optimiser and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e6a2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Update the initialisation to incorporate GPU training\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Specify the weight decay for each parameter\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"lr\": LEARNING_RATE\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"lr\": LEARNING_RATE\n",
    "    },\n",
    "]\n",
    "\n",
    "# Initialise the optimizer\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "# Initialise the learning rate scheduler\n",
    "num_training_steps = len(train_dataloader) * TRAINING_EPOCHS\n",
    "num_warmup_steps = WARMUP_PROPORTION * num_training_steps\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
    "                                               num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0bca55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "832895f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Initialise the scaler for mixed precision training\n",
    "scaler = torch.amp.GradScaler(device=DEVICE) if MIXED_PRECISION_TRAINING else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735e858",
   "metadata": {},
   "source": [
    "##### 3.1. Training and evaluating the model\n",
    "\n",
    "Here we implement a training step and evaluation function for updating and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3761c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Update the functions to incorporate GPU training\n",
    "# Exercise 3: Update the functions to incorporate mixed precision training\n",
    "\n",
    "def training_step(batch):\n",
    "    \"\"\"\n",
    "    Function to perform a training step.\n",
    "    \n",
    "    Args:\n",
    "        batch (dict): Batch of data.\n",
    "    Returns:\n",
    "        loss (torch.Tensor): Loss for the batch.\n",
    "    \"\"\"\n",
    "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "    \n",
    "    if MIXED_PRECISION_TRAINING:\n",
    "        with torch.amp.autocast(device_type=DEVICE):\n",
    "            loss = model(**batch).loss\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss = model(**batch).loss\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "        # Update step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # common steps\n",
    "    lr_scheduler.step()\n",
    "    model.zero_grad()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    \"\"\"\n",
    "    Function to evaluate the model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): Dataloader for the data.\n",
    "    Returns:\n",
    "        f1 (float): F1 Score for the model.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    predictions = list()\n",
    "    labels = list()\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader, desc=\"Eval\"):\n",
    "        # Forward pass data\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        if MIXED_PRECISION_TRAINING:\n",
    "            with torch.no_grad(), torch.amp.autocast(device_type=DEVICE):\n",
    "                logits = model(**batch).logits.detach().cpu()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits = model(**batch).logits.detach().cpu()\n",
    "        pred = logits.argmax(-1)\n",
    "\n",
    "        predictions.append(pred.reshape(-1))\n",
    "        labels.append(batch['labels'].cpu().reshape(-1))\n",
    "\n",
    "    # Reset model to training mode\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    # Compute the F1 Score\n",
    "    predictions = torch.concat(predictions, 0)\n",
    "    labels = torch.concat(labels, 0)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8028fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 6547/6547 [04:50<00:00, 22.57it/s, Loss=0.508] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 290.07 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 342/342 [00:03<00:00, 94.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.9201945595388218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 6547/6547 [04:49<00:00, 22.58it/s, Loss=0.049]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed in 289.92 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 342/342 [00:03<00:00, 94.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.9273661041819515\n",
      "Mode: Mixed Precision\n",
      "Training completed in 587.21 seconds.\n",
      "Mean Validation F1 Score over epochs: 0.9237803318603867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "training_start_time = time.time()\n",
    "# Prepare model for training\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for e in range(TRAINING_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    iterator = tqdm.tqdm(train_dataloader, desc=f\"Epoch {e+1}/{TRAINING_EPOCHS}\")\n",
    "    # Perform an epoch of training\n",
    "    for batch in iterator:\n",
    "        loss = training_step(batch)\n",
    "        iterator.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {e+1} completed in {epoch_end_time - epoch_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Evaluate the model and report F1 Score\n",
    "    f1 = evaluate(validation_dataloader)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Validation F1 Score: {f1}\")\n",
    "\n",
    "print(f\"Mode: {'Mixed Precision' if MIXED_PRECISION_TRAINING else 'GPU'}\")\n",
    "training_end_time = time.time()\n",
    "print(f\"Training completed in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"Mean Validation F1 Score over epochs: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9f8743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXED_PRECISION_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2468be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 6547/6547 [09:17<00:00, 11.74it/s, Loss=0.0054] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 557.62 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 342/342 [00:08<00:00, 38.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.9273661041819515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 6547/6547 [09:17<00:00, 11.74it/s, Loss=0.0173] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed in 557.62 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 342/342 [00:08<00:00, 38.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.9273661041819515\n",
      "Mode: GPU\n",
      "Training completed in 1133.10 seconds.\n",
      "Mean Validation F1 Score over epochs: 0.9273661041819515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "training_start_time = time.time()\n",
    "# Prepare model for training\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for e in range(TRAINING_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    iterator = tqdm.tqdm(train_dataloader, desc=f\"Epoch {e+1}/{TRAINING_EPOCHS}\")\n",
    "    # Perform an epoch of training\n",
    "    for batch in iterator:\n",
    "        loss = training_step(batch)\n",
    "        iterator.set_postfix({'Loss': loss.item()})\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    print(f\"Epoch {e+1} completed in {epoch_end_time - epoch_start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Evaluate the model and report F1 Score\n",
    "    f1 = evaluate(validation_dataloader)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"Validation F1 Score: {f1}\")\n",
    "\n",
    "print(f\"Mode: {'Mixed Precision' if MIXED_PRECISION_TRAINING else 'GPU'}\")\n",
    "training_end_time = time.time()\n",
    "print(f\"Training completed in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"Mean Validation F1 Score over epochs: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d5b54",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Study the code used for training this entailment Roberta model.\n",
    "2. Make the neccesary changes to train this model on the GPU rather than the CPU.\n",
    "3. Make the neccesary changes to train this model using mixed precision training (see the [documentation](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/) for more details on this).\n",
    "4. Discuss the differences in training time and model performance for the three approaches (CPU, GPU and GPU Mixed precision)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
