# Pre-LayerNorm Transformer Training Configuration
model:
  vocab_size: 37000  # Will be set dynamically from tokenizer
  d_model: 512
  n_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  max_len: 128
  dim_feedforward: 2048
  dropout: 0.1

training:
  num_steps: 200000  # Increased to see ~2.5 epochs
  batch_size: 128     
  learning_rate: 0.0005  # Standard for d_model=512
  weight_decay: 0.01
  warmup_steps: 4000     # 2% of training steps
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_eps: 0.000000001
  max_grad_norm: 1.0

  log_interval: 100
  val_interval: 5000
  checkpoint_interval: 10000

data:
  train_path: "data/cleaned_wmt17_de_en_split_train.json"
  val_path: "data/cleaned_wmt17_de_en_split_validation.json"
  test_path: "data/cleaned_wmt17_de_en_split_test.json"
  tokenizer_path: "data/bpe_tokenizer"
  num_train_samples: null  # null = use full dataset
  num_val_samples: null
  num_test_samples: null
  max_src_len: 128
  max_tgt_len: 128

generation:
  max_length: 128
  method: "beam"  # "greedy" or "beam"
  beam_size: 5  
  length_penalty: 0.6  # (0.0-1.5 typical range)

paths:
  checkpoint_dir: "checkpoints_preln"
  log_dir: "logs_preln"
  output_dir: "outputs_preln"

wandb:
  enabled: true
  project: "transformer-translation-preln"

ablation:
  log_timing: true

seed: 42
