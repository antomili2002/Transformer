# Pre-LayerNorm Transformer Training Configuration
# Optimized config for stable Pre-LN training on WMT17 DE-EN dataset

model:
  vocab_size: 37000  # Will be set dynamically from tokenizer
  d_model: 768
  n_heads: 12
  num_encoder_layers: 8
  num_decoder_layers: 8
  max_len: 128
  dim_feedforward: 3072
  dropout: 0.1

training:
  num_steps: 200000  
  batch_size: 32    
  learning_rate: 0.001  
  weight_decay: 0.01     
  warmup_steps: 4000
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_eps: 0.000000001
  max_grad_norm: 1.0

  log_interval: 100
  val_interval: 2000    # More frequent validation (was 5000)
  checkpoint_interval: 10000

data:
  train_path: "data/cleaned_wmt17_de_en_split_train.json"
  val_path: "data/cleaned_wmt17_de_en_split_validation.json"
  test_path: "data/cleaned_wmt17_de_en_split_test.json"
  tokenizer_path: "data/bpe_tokenizer"
  num_train_samples: null  # null = use full dataset
  num_val_samples: null
  num_test_samples: null
  max_src_len: 128
  max_tgt_len: 128

generation:
  max_length: 128
  method: "greedy"  # greedy, beam, sampling

paths:
  checkpoint_dir: "checkpoints_preln_big"  # Separate dir for Pre-LN checkpoints
  log_dir: "logs_preln_big"
  output_dir: "outputs_preln_big"

wandb:
  enabled: true
  project: "transformer-translation-preln"

ablation:
  log_timing: true

seed: 42
